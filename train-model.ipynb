{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12210294,"sourceType":"datasetVersion","datasetId":7691990}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\n\n# Set seed\ntorch.manual_seed(42)\n\n# Load and preprocess dataset\ndf = pd.read_csv('/kaggle/input/dataset12/merged_cleaned-data.csv')\ndf = df[['query', 'response']].dropna()\n\n# Tokenization\ndef tokenize(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9\\s]', '', text)\n    return text.split()\n\n# Build vocab\ntokenized_inputs = [tokenize(s) for s in df['query']]\ntokenized_outputs = [tokenize(s) for s in df['response']]\nall_tokens = sum(tokenized_inputs + tokenized_outputs, [])\nvocab = ['<pad>', '<sos>', '<eos>', '<unk>'] + sorted(set(all_tokens))\nword2idx = {w: i for i, w in enumerate(vocab)}\nidx2word = {i: w for w, i in word2idx.items()}\nvocab_size = len(word2idx)\n\n\nvocab = ['<pad>', '<sos>', '<eos>', '<unk>'] + sorted(set(all_tokens))\nword2idx = {w: i for i, w in enumerate(vocab)}\nidx2word = {i: w for w, i in word2idx.items()}\n\n# Encode\nMAX_LEN = 20\n\ndef encode(tokens):\n    ids = [word2idx.get(w, word2idx['<unk>']) for w in tokens]\n    return [word2idx['<sos>']] + ids[:MAX_LEN-2] + [word2idx['<eos>']]\n\nclass ChatDataset(Dataset):\n    def __init__(self, queries, responses):\n        self.inputs = [encode(tokenize(s)) for s in queries]\n        self.outputs = [encode(tokenize(s)) for s in responses]\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input_ids = self.inputs[idx]\n        target_ids = self.outputs[idx]\n\n        input_ids += [word2idx['<pad>']] * (MAX_LEN - len(input_ids))\n        target_ids += [word2idx['<pad>']] * (MAX_LEN - len(target_ids))\n\n        return torch.tensor(input_ids), torch.tensor(target_ids)\n\n# DataLoader\ntrain_data, val_data = train_test_split(df, test_size=0.1, random_state=42)\ntrain_ds = ChatDataset(train_data['query'], train_data['response'])\nval_ds = ChatDataset(val_data['query'], val_data['response'])\ntrain_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=16)\n\n# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)].to(x.device)\n\n# Transformer Model\nclass TransformerChatbot(nn.Module):\n    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_ff=512):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = PositionalEncoding(d_model)\n        self.transformer = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n            dim_feedforward=dim_ff,\n            dropout=0.1,\n            batch_first=True\n        )\n        self.fc_out = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n\n    def forward(self, src, tgt):\n        src_emb = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)).to(src.device)\n        tgt_emb = self.embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)).to(tgt.device)\n        src_emb = self.pos_encoder(src_emb)\n        tgt_emb = self.pos_encoder(tgt_emb)\n\n        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n        src_key_padding_mask = (src == word2idx['<pad>'])\n        tgt_key_padding_mask = (tgt == word2idx['<pad>'])\n\n        out = self.transformer(\n            src_emb, tgt_emb,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask,\n            memory_key_padding_mask=src_key_padding_mask\n        )\n        return self.fc_out(out)\n\n# Initialize Model\nmodel = TransformerChatbot(vocab_size)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\nloss_fn = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training Loop with Accuracy\nEPOCHS = 100\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_tokens = 0\n\n    for src, tgt in train_dl:\n        src, tgt = src.to(device), tgt.to(device)\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        output = model(src, tgt_input)\n        output = output[:, :tgt_output.size(1), :]\n        output = output.reshape(-1, output.shape[-1])\n        tgt_output = tgt_output.reshape(-1)\n\n        loss = loss_fn(output, tgt_output)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        preds = output.argmax(dim=1)\n        mask = tgt_output != word2idx['<pad>']\n        total_correct += (preds == tgt_output)[mask].sum().item()\n        total_tokens += mask.sum().item()\n\n    avg_loss = total_loss / len(train_dl)\n    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f} | Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T19:30:16.684179Z","iopub.execute_input":"2025-06-18T19:30:16.684467Z","iopub.status.idle":"2025-06-18T19:56:48.584157Z","shell.execute_reply.started":"2025-06-18T19:30:16.684443Z","shell.execute_reply":"2025-06-18T19:56:48.583452Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 6.2097 | Accuracy: 20.37%\nEpoch 2 | Train Loss: 4.8090 | Accuracy: 31.90%\nEpoch 3 | Train Loss: 4.4148 | Accuracy: 35.83%\nEpoch 4 | Train Loss: 4.1596 | Accuracy: 37.98%\nEpoch 5 | Train Loss: 3.9699 | Accuracy: 39.67%\nEpoch 6 | Train Loss: 3.8207 | Accuracy: 40.84%\nEpoch 7 | Train Loss: 3.6891 | Accuracy: 42.15%\nEpoch 8 | Train Loss: 3.5720 | Accuracy: 42.99%\nEpoch 9 | Train Loss: 3.4628 | Accuracy: 44.14%\nEpoch 10 | Train Loss: 3.3633 | Accuracy: 44.90%\nEpoch 11 | Train Loss: 3.2686 | Accuracy: 45.86%\nEpoch 12 | Train Loss: 3.1768 | Accuracy: 46.74%\nEpoch 13 | Train Loss: 3.0873 | Accuracy: 47.56%\nEpoch 14 | Train Loss: 3.0062 | Accuracy: 48.36%\nEpoch 15 | Train Loss: 2.9272 | Accuracy: 49.22%\nEpoch 16 | Train Loss: 2.8418 | Accuracy: 50.15%\nEpoch 17 | Train Loss: 2.7639 | Accuracy: 51.06%\nEpoch 18 | Train Loss: 2.6871 | Accuracy: 51.85%\nEpoch 19 | Train Loss: 2.6149 | Accuracy: 52.82%\nEpoch 20 | Train Loss: 2.5444 | Accuracy: 53.67%\nEpoch 21 | Train Loss: 2.4683 | Accuracy: 54.58%\nEpoch 22 | Train Loss: 2.3958 | Accuracy: 55.61%\nEpoch 23 | Train Loss: 2.3238 | Accuracy: 56.39%\nEpoch 24 | Train Loss: 2.2541 | Accuracy: 57.45%\nEpoch 25 | Train Loss: 2.1902 | Accuracy: 58.30%\nEpoch 26 | Train Loss: 2.1233 | Accuracy: 59.20%\nEpoch 27 | Train Loss: 2.0615 | Accuracy: 60.18%\nEpoch 28 | Train Loss: 1.9960 | Accuracy: 61.18%\nEpoch 29 | Train Loss: 1.9273 | Accuracy: 62.11%\nEpoch 30 | Train Loss: 1.8646 | Accuracy: 63.20%\nEpoch 31 | Train Loss: 1.8047 | Accuracy: 64.07%\nEpoch 32 | Train Loss: 1.7427 | Accuracy: 65.20%\nEpoch 33 | Train Loss: 1.6876 | Accuracy: 65.93%\nEpoch 34 | Train Loss: 1.6277 | Accuracy: 66.89%\nEpoch 35 | Train Loss: 1.5746 | Accuracy: 67.85%\nEpoch 36 | Train Loss: 1.5167 | Accuracy: 68.93%\nEpoch 37 | Train Loss: 1.4652 | Accuracy: 69.70%\nEpoch 38 | Train Loss: 1.4099 | Accuracy: 70.69%\nEpoch 39 | Train Loss: 1.3595 | Accuracy: 71.70%\nEpoch 40 | Train Loss: 1.3130 | Accuracy: 72.51%\nEpoch 41 | Train Loss: 1.2607 | Accuracy: 73.50%\nEpoch 42 | Train Loss: 1.2133 | Accuracy: 74.48%\nEpoch 43 | Train Loss: 1.1654 | Accuracy: 75.45%\nEpoch 44 | Train Loss: 1.1173 | Accuracy: 76.42%\nEpoch 45 | Train Loss: 1.0742 | Accuracy: 77.28%\nEpoch 46 | Train Loss: 1.0364 | Accuracy: 77.93%\nEpoch 47 | Train Loss: 0.9915 | Accuracy: 78.92%\nEpoch 48 | Train Loss: 0.9515 | Accuracy: 79.69%\nEpoch 49 | Train Loss: 0.9170 | Accuracy: 80.33%\nEpoch 50 | Train Loss: 0.8751 | Accuracy: 81.34%\nEpoch 51 | Train Loss: 0.8455 | Accuracy: 81.88%\nEpoch 52 | Train Loss: 0.8079 | Accuracy: 82.88%\nEpoch 53 | Train Loss: 0.7741 | Accuracy: 83.44%\nEpoch 54 | Train Loss: 0.7435 | Accuracy: 84.21%\nEpoch 55 | Train Loss: 0.7187 | Accuracy: 84.64%\nEpoch 56 | Train Loss: 0.6854 | Accuracy: 85.40%\nEpoch 57 | Train Loss: 0.6546 | Accuracy: 86.08%\nEpoch 58 | Train Loss: 0.6325 | Accuracy: 86.53%\nEpoch 59 | Train Loss: 0.6061 | Accuracy: 87.13%\nEpoch 60 | Train Loss: 0.5787 | Accuracy: 87.79%\nEpoch 61 | Train Loss: 0.5597 | Accuracy: 88.16%\nEpoch 62 | Train Loss: 0.5383 | Accuracy: 88.73%\nEpoch 63 | Train Loss: 0.5186 | Accuracy: 89.12%\nEpoch 64 | Train Loss: 0.4985 | Accuracy: 89.53%\nEpoch 65 | Train Loss: 0.4844 | Accuracy: 89.82%\nEpoch 66 | Train Loss: 0.4659 | Accuracy: 90.35%\nEpoch 67 | Train Loss: 0.4450 | Accuracy: 90.78%\nEpoch 68 | Train Loss: 0.4316 | Accuracy: 90.95%\nEpoch 69 | Train Loss: 0.4121 | Accuracy: 91.45%\nEpoch 70 | Train Loss: 0.4028 | Accuracy: 91.61%\nEpoch 71 | Train Loss: 0.3860 | Accuracy: 91.98%\nEpoch 72 | Train Loss: 0.3748 | Accuracy: 92.17%\nEpoch 73 | Train Loss: 0.3633 | Accuracy: 92.40%\nEpoch 74 | Train Loss: 0.3495 | Accuracy: 92.83%\nEpoch 75 | Train Loss: 0.3377 | Accuracy: 93.09%\nEpoch 76 | Train Loss: 0.3303 | Accuracy: 93.15%\nEpoch 77 | Train Loss: 0.3188 | Accuracy: 93.49%\nEpoch 78 | Train Loss: 0.3099 | Accuracy: 93.64%\nEpoch 79 | Train Loss: 0.3009 | Accuracy: 93.76%\nEpoch 80 | Train Loss: 0.2899 | Accuracy: 94.05%\nEpoch 81 | Train Loss: 0.2834 | Accuracy: 94.22%\nEpoch 82 | Train Loss: 0.2766 | Accuracy: 94.33%\nEpoch 83 | Train Loss: 0.2711 | Accuracy: 94.40%\nEpoch 84 | Train Loss: 0.2631 | Accuracy: 94.70%\nEpoch 85 | Train Loss: 0.2557 | Accuracy: 94.76%\nEpoch 86 | Train Loss: 0.2516 | Accuracy: 94.86%\nEpoch 87 | Train Loss: 0.2440 | Accuracy: 94.98%\nEpoch 88 | Train Loss: 0.2387 | Accuracy: 95.11%\nEpoch 89 | Train Loss: 0.2338 | Accuracy: 95.14%\nEpoch 90 | Train Loss: 0.2297 | Accuracy: 95.34%\nEpoch 91 | Train Loss: 0.2249 | Accuracy: 95.35%\nEpoch 92 | Train Loss: 0.2202 | Accuracy: 95.50%\nEpoch 93 | Train Loss: 0.2182 | Accuracy: 95.58%\nEpoch 94 | Train Loss: 0.2105 | Accuracy: 95.74%\nEpoch 95 | Train Loss: 0.2074 | Accuracy: 95.84%\nEpoch 96 | Train Loss: 0.2020 | Accuracy: 95.92%\nEpoch 97 | Train Loss: 0.1990 | Accuracy: 96.01%\nEpoch 98 | Train Loss: 0.1977 | Accuracy: 95.89%\nEpoch 99 | Train Loss: 0.1900 | Accuracy: 96.20%\nEpoch 100 | Train Loss: 0.1891 | Accuracy: 96.25%\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Save the model\ntorch.save(model.state_dict(), \"transformer_chatbot6.pth\")\nprint(\"Model saved to transformer_chatbot.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T19:57:12.043484Z","iopub.execute_input":"2025-06-18T19:57:12.044211Z","iopub.status.idle":"2025-06-18T19:57:12.149206Z","shell.execute_reply.started":"2025-06-18T19:57:12.044182Z","shell.execute_reply":"2025-06-18T19:57:12.148386Z"}},"outputs":[{"name":"stdout","text":"Model saved to transformer_chatbot.pth\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import json\n\n# Save vocabulary\nwith open(\"word2idx.json\", \"w\") as f:\n    json.dump(word2idx, f)\n\nwith open(\"idx2word.json\", \"w\") as f:\n    json.dump(idx2word, f)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T19:57:19.154509Z","iopub.execute_input":"2025-06-18T19:57:19.154820Z","iopub.status.idle":"2025-06-18T19:57:19.196381Z","shell.execute_reply.started":"2025-06-18T19:57:19.154796Z","shell.execute_reply":"2025-06-18T19:57:19.195848Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create a download link\nFileLink(\"transformer_chatbot6.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T19:57:29.397138Z","iopub.execute_input":"2025-06-18T19:57:29.397424Z","iopub.status.idle":"2025-06-18T19:57:29.402177Z","shell.execute_reply.started":"2025-06-18T19:57:29.397401Z","shell.execute_reply":"2025-06-18T19:57:29.401578Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/transformer_chatbot6.pth","text/html":"<a href='transformer_chatbot6.pth' target='_blank'>transformer_chatbot6.pth</a><br>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import pickle\n\n# Define a path to save the vocab file\nvocab_save_path7 = 'vocab.pkl'  # Change path if needed\n\n# Save the word2idx and idx2word\nwith open(vocab_save_path7, 'wb') as f:\n    pickle.dump((word2idx, idx2word), f)\n\nprint(f\"Vocabulary saved to {vocab_save_path7}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T19:57:50.531530Z","iopub.execute_input":"2025-06-18T19:57:50.531783Z","iopub.status.idle":"2025-06-18T19:57:50.542867Z","shell.execute_reply.started":"2025-06-18T19:57:50.531767Z","shell.execute_reply":"2025-06-18T19:57:50.542216Z"}},"outputs":[{"name":"stdout","text":"Vocabulary saved to vocab.pkl\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create a download link\nFileLink('vocab.pkl')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T19:57:57.057911Z","iopub.execute_input":"2025-06-18T19:57:57.058696Z","iopub.status.idle":"2025-06-18T19:57:57.063819Z","shell.execute_reply.started":"2025-06-18T19:57:57.058644Z","shell.execute_reply":"2025-06-18T19:57:57.063051Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/vocab.pkl","text/html":"<a href='vocab.pkl' target='_blank'>vocab.pkl</a><br>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}