{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "567a5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd810dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab\n",
    "with open(\"C:/Users/BIBHAV KUMAR/Desktop/Chatbot_Project/Revised_Model/vocab.pkl\", \"rb\") as f:\n",
    "    word2idx, idx2word = pickle.load(f)\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "MAX_LEN = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b5f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and Encode\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "def encode(tokens, word2idx, max_len=MAX_LEN):\n",
    "    ids = [word2idx.get(w, word2idx[\"<unk>\"]) for w in tokens]\n",
    "    return [word2idx[\"<sos>\"]] + ids[:max_len - 2] + [word2idx[\"<eos>\"]]\n",
    "\n",
    "def decode(ids, idx2word):\n",
    "    words = [idx2word.get(i, \"<unk>\") for i in ids]\n",
    "    return \" \".join([w for w in words if w not in [\"<sos>\", \"<eos>\", \"<pad>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db201d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67cbd9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class TransformerChatbot(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_ff=512):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_ff,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)).to(src.device)\n",
    "        tgt_emb = self.embedding(tgt) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)).to(tgt.device)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        src_key_padding_mask = (src == word2idx['<pad>'])\n",
    "        tgt_key_padding_mask = (tgt == word2idx['<pad>'])\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_emb, tgt_emb,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4a5a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerChatbot(\n",
       "  (embedding): Embedding(17634, 256)\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=256, out_features=17634, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "model = TransformerChatbot(vocab_size)\n",
    "model.load_state_dict(torch.load(\"C:/Users/BIBHAV KUMAR/Desktop/Chatbot_Project/Revised_Model/transformer_chatbot6.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e56b2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Function\n",
    "def generate_response(model, input_text, max_len=MAX_LEN):\n",
    "    tokens = tokenize(input_text)\n",
    "    input_ids = encode(tokens, word2idx, max_len)\n",
    "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "\n",
    "    output_ids = [word2idx[\"<sos>\"]]\n",
    "    for _ in range(max_len):\n",
    "        output_tensor = torch.tensor(output_ids).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor, output_tensor)\n",
    "        next_token_logits = output[0, -1, :]\n",
    "        next_token = next_token_logits.argmax().item()\n",
    "        output_ids.append(next_token)\n",
    "        if next_token == word2idx[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    return decode(output_ids, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Chatbot is ready! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIBHAV KUMAR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n",
      "C:\\Users\\BIBHAV KUMAR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BIBHAV KUMAR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: hey mate i am doing just fine please lets move ahead with the queries i can help you\n",
      "Chatbot: please contact our customer service immediately to report your lost card and request a replacement\n",
      "Chatbot: flu symptoms include fever cough sore throat body aches and fatigue\n",
      "Chatbot: i really hope it rains today\n",
      "Chatbot: that is true the weather is constantly changing\n",
      "Chatbot: i am mike\n",
      "Chatbot: i am mike\n",
      "Chatbot: i am mike\n"
     ]
    }
   ],
   "source": [
    "# Chat Loop\n",
    "print(\"ðŸ¤– Chatbot is ready! Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "    response = generate_response(model, user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0c472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
